{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4164a84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/hlt/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from filelock import FileLock\n",
    "from transformers import AdamW, get_scheduler, set_seed\n",
    "\n",
    "from transformers.file_utils import is_offline_mode\n",
    "from transformers.utils.versions import require_version\n",
    "\n",
    "from args import parse_args\n",
    "from data_loader import raw_data_loader, data_processor\n",
    "from model_loader import model_loader\n",
    "from rouge_s import py_rouge_scores\n",
    "from scoring import bleu_scores, meteor_scores\n",
    "from utils import label_smoothed_nll_loss, postprocess_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5935fa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    output_dir = \"./output/sample\"\n",
    "    train_file = \"./data/samsum/train_small.csv\"\n",
    "    validation_file = \"./data/samsum/val_small.csv\"\n",
    "    test_file = \"./data/samsum/test_small.csv\"\n",
    "    text_column = \"dialogue\"\n",
    "    summary_column = \"summary\"\n",
    "#     model_name_or_path = \"t5-base\"\n",
    "    model_name_or_path = \"./output/run_cngen_turns_samsum_bart_base/best\"\n",
    "    model_type = \"bart\"\n",
    "    source_prefix = \"\"\n",
    "    max_source_length = 1024\n",
    "    min_target_length = 1\n",
    "    max_target_length = 128\n",
    "    learning_rate = 5e-5\n",
    "    weight_decay = 1e-3\n",
    "    label_smoothing = 0.1\n",
    "    length_penalty = 1.0\n",
    "    num_train_epochs = 4\n",
    "    per_device_train_batch_size = 1\n",
    "    gradient_accumulation_steps = 16\n",
    "    per_device_eval_batch_size = 1\n",
    "    per_device_test_batch_size = 1\n",
    "    num_warmup_steps = 0\n",
    "    cache_dir = \"./output/cache\"\n",
    "    overwrite_cache = True\n",
    "    seed = 12345\n",
    "    \n",
    "    ignore_pad_token_for_loss = True\n",
    "    preprocessing_num_workers = None\n",
    "    overwrite_cache = None\n",
    "    num_beams = None\n",
    "    pad_to_max_length = True\n",
    "    config_name = None\n",
    "    tokenizer_name = \"t5-base\"\n",
    "    use_slow_tokenizer = True\n",
    "    max_train_steps = None\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    shuffle = False\n",
    "    \n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678569f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =  =  =  =  =  =  =  =  =  = Logging Setup =  =  =  =  =  =  =  =  =  =  =  = \n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4951322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =  =  =  =  =  =  =  =  =  = Pre-check Package Info =  =  =  =  =  =  =  =  =  =  =  = \n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    if is_offline_mode():\n",
    "        raise LookupError(\n",
    "            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n",
    "        )\n",
    "    with FileLock(\".lock\") as lock:\n",
    "        nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883b4724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/15/2022 09:07:17 - INFO - root - *** Parameters ***\n",
      "07/15/2022 09:07:17 - INFO - root - \n",
      "07/15/2022 09:07:17 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Use FP16 precision: False\n",
      "\n",
      "loading configuration file ./output/run_cngen_turns_samsum_bart_base/best/config.json\n",
      "Model config BartConfig {\n",
      "  \"_name_or_path\": \"./output/run_cngen_turns_samsum_bart_base/best\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_bos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 128,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"min_length\": 1,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 128,\n",
      "      \"min_length\": 1,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_cnn\": {\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 142,\n",
      "      \"min_length\": 56,\n",
      "      \"num_beams\": 4\n",
      "    },\n",
      "    \"summarization_xsum\": {\n",
      "      \"length_penalty\": 1.0,\n",
      "      \"max_length\": 62,\n",
      "      \"min_length\": 11,\n",
      "      \"num_beams\": 6\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at ./output/cache/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/t5-base/resolve/main/spiece.model from cache at ./output/cache/684a47ca6257e4ca71f0037771464c5b323e945fbc58697d2fad8a7dd1a2f8ba.3b69006860e7b5d0a63ffdddc01ddcd6b7c318a6f4fd793596552c741734c62d\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer.json from cache at ./output/cache/90de37880b5ff5ac7ab70ff0bd369f207e9b74133fa153c163d14c5bb0116207.8627f1bd5d270a9fd2e5a51c8bec3223896587cc3cfe13edeabb0992ab43c529\n",
      "loading file https://huggingface.co/t5-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/t5-base/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/t5-base/resolve/main/config.json from cache at ./output/cache/91e9fe874e06c44883b535d6c950b8b89d6eaa3298d8e7fb3b2c78039e9f8b7b.66b9637a52aa11e9285cdd6e668cc0df14b3bcf0b6674cf3ba5353c542649637\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-base\",\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 3072,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.13.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./output/run_cngen_turns_samsum_bart_base/best/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BartForConditionalGeneration.\n",
      "\n",
      "All the weights of BartForConditionalGeneration were initialized from the model checkpoint at ./output/run_cngen_turns_samsum_bart_base/best.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n",
      "Configuration saved in ./output/sample/start/config.json\n",
      "Model weights saved in ./output/sample/start/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/sample/start/tokenizer_config.json\n",
      "Special tokens file saved in ./output/sample/start/special_tokens_map.json\n",
      "Running tokenizer on dataset: 100%|█████████████| 12/12 [00:04<00:00,  2.47ba/s]\n",
      "Running tokenizer on dataset: 100%|███████████████| 2/2 [00:00<00:00,  4.18ba/s]\n",
      "Running tokenizer on dataset: 100%|███████████████| 2/2 [00:00<00:00,  3.81ba/s]\n",
      "07/15/2022 09:07:39 - INFO - __main__ - Sample 6825 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [20698, 3, 18, 10149, 7, 9, 11, 12020, 54, 31, 17, 253, 8, 9972, 79, 174, 21, 5721, 31, 7, 794, 5, 16471, 16355, 135, 57, 3125, 11, 79, 3492, 95, 16, 7623, 5, 1193, 6327, 3, 18, 10149, 7, 9, 10, 410, 841, 9268, 8, 3, 1572, 17, 9972, 24, 130, 16, 69, 16, 2689, 58, 12020, 10, 3, 23, 47, 81, 12, 987, 25, 8, 337, 5, 54, 31, 17, 253, 34, 16715, 8513, 10, 125, 9972, 10149, 7, 9, 10, 25692, 1622, 34, 114, 220, 1274, 977, 21038, 23, 9, 10, 3, 13133, 8, 794, 19, 5721, 6, 269, 58, 10149, 7, 9, 10, 4273, 3967, 6, 24, 31, 7, 572, 3, 23, 31, 51, 1119, 12, 253, 175, 3, 89, 4636, 53, 9972, 21038, 23, 9, 10, 3, 7, 10536, 6, 3, 23, 1114, 12, 946, 135, 469, 12020, 10, 168, 6, 230, 25, 54, 31, 17, 8513, 10, 103, 25, 3413, 43, 273, 7594, 21, 28288, 12, 39, 1045, 1115, 10149, 7, 9, 10, 207, 822, 5, 27, 317, 128, 146, 51, 18, 26, 440, 16355, 135, 45, 70, 293, 905, 12020, 10, 1506, 4923, 3, 18, 116, 3, 63, 9, 9268, 3, 7, 51, 189, 45, 39, 1045, 16, 2689, 6, 34, 92, 7671, 16355, 45, 69, 563, 16, 2689, 21038, 23, 9, 10, 150, 3, 7, 10536, 3413, 233, 27, 214, 24, 10149, 7, 9, 10, 17945, 68, 841, 2087, 410, 59, 19034, 10, 3, 32, 51, 122, 147, 60, 2708, 53, 231, 58, 3, 23, 31, 162, 7122, 7623, 11, 34, 31, 7, 341, 132, 5527, 7, 3, 18, 489, 16778, 3, 18, 10149, 7, 9, 2035, 8478, 3, 18, 3547, 9273, 3, 18, 1902, 312, 1725, 189, 3, 18, 301, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [17945, 68, 572, 103, 62, 43, 12, 9635, 34, 45, 7623, 6, 131, 278, 31, 17, 36, 46, 25851, 16, 8, 166, 286, 1, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}.\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = Main Process = = = = = = = = = = = = = = = = = =\n",
    "# Display Parameters\n",
    "logging.info(\"*** Parameters ***\")\n",
    "for item, value in vars(args).items():\n",
    "    logging.info(\"{}: {}\".format(item, value))\n",
    "logging.info(\"\")\n",
    "\n",
    "# Initialize the accelerator. The accelerator will handle device placement for us.\n",
    "accelerator = Accelerator()\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    #datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    #datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "# If passed along, set the training seed now.\n",
    "if args.seed is not None:\n",
    "    set_seed(args.seed)\n",
    "    torch.backends.cudnn.enabled = False \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# load raw dataset\n",
    "raw_datasets = raw_data_loader(args)\n",
    "\n",
    "# load model (config, tokenizer, s2s model)\n",
    "config, tokenizer, model = model_loader(accelerator, logger, args)\n",
    "\n",
    "# data processor (for DataLoader)\n",
    "dataloader, processed_dataset = data_processor(logger, args, accelerator, raw_datasets, tokenizer, model)\n",
    "train_dataloader, eval_dataloader, test_dataloader = dataloader\n",
    "train_dataset, _, _ = processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405bbcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = Training Preparation = = =\n",
    "# Split weights in two groups, one with weight decay and the other not.\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "no_decay_emb_matrix = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay_emb_matrix)],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "model, optimizer, train_dataloader, eval_dataloader, test_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, test_dataloader\n",
    ")\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "else:\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4a5b2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/17/2022 14:52:16 - INFO - __main__ - ***** Running training *****\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Num examples = 11071\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Num Epochs = 4\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Gradient Accumulation steps = 16\n",
      "06/17/2022 14:52:16 - INFO - __main__ -   Total optimization steps = 2768\n",
      "Training:   0%|                                        | 0/2768 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = Train = = = = = = = = = = = = = = = = = = =\n",
    "total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "\n",
    "# Only show the progress bar once on each machine.\n",
    "progress_bar = tqdm(range(args.max_train_steps), desc=\"Training: \", disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "\n",
    "val_results = []\n",
    "acc_losses  = []\n",
    "best_r2_f1  = None\n",
    "best_epoch  = 0\n",
    "\n",
    "if args.model_type == 'bart' or args.model_type == 't5':\n",
    "    task_specific_params = model.config.task_specific_params\n",
    "    params = task_specific_params.get('summarization', {})\n",
    "    params['min_length'] = args.min_target_length\n",
    "    params['max_length'] = args.max_target_length\n",
    "    params['length_penalty'] = args.length_penalty\n",
    "    model.config.update(params)\n",
    "else:\n",
    "    raise ValueError('{} model type not implemented'.format(args.model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7cf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =  =  =  =  =  =  =  =  =  =  =  =  =  =  =  = EVAL =  =  =  =  =  =  =  =  =  =  =  =  =  =  = \n",
    "def evaluate_model(index):\n",
    "\n",
    "    loop_count = 0\n",
    "    \n",
    "    model.eval()\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "\n",
    "        loop_count += 1\n",
    "\n",
    "        if loop_count != index:\n",
    "            continue\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = batch[\"input_ids\"]\n",
    "            mask = batch[\"attention_mask\"]\n",
    "            input_text = tokenizer.batch_decode(inputs, skip_special_tokens=True)[0] + \"\\n\"\n",
    "\n",
    "            count = 0\n",
    "            while True:         \n",
    "                # batch[\"input_ids\"] is the input string turned into tokens\n",
    "                # generated_tokens is the predicted sentence\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    inputs,\n",
    "                    attention_mask=mask\n",
    "                )\n",
    "\n",
    "                generated_tokens = accelerator.pad_across_processes(\n",
    "                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "\n",
    "                generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "                dialogue_output = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "                if dialogue_output == 'end_of_dialogue' or count > 15:\n",
    "                    break\n",
    "\n",
    "                input_text = input_text + dialogue_output + \"\\n\"\n",
    "                tokenized = tokenizer([input_text], max_length=args.max_source_length, padding='max_length', truncation=True)\n",
    "                inputs, mask = tokenized[\"input_ids\"], tokenized[\"attention_mask\"]\n",
    "                inputs, mask = torch.tensor(inputs), torch.tensor(mask)\n",
    "                inputs = inputs.to(device=\"cuda:0\")\n",
    "                mask = mask.to(device=\"cuda:0\")\n",
    "\n",
    "                count += 1\n",
    "            \n",
    "            print(\"========================== Generated Dialogue from Summary ====================== \")\n",
    "            print(input_text)\n",
    "\n",
    "            labels = batch[\"labels\"]\n",
    "            if not args.pad_to_max_length:\n",
    "                # If we did not pad to max length, we need to pad the labels too\n",
    "                labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "            if args.ignore_pad_token_for_loss:\n",
    "                # Replace -100 in the labels as we can't decode them.\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "            \n",
    "            print(\"========================== Original Dialogue ================================== \")\n",
    "            print(decoded_labels[0])\n",
    "\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "behavioral-transsexual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================== Generated Dialogue from Summary ====================== \n",
      "generate: Summary - Marta is grocery shopping for dinner. She and Nick will make lasagne. Dialogue - Marta: Hi, I'm at the supermarket now to make some shopping for todays dinner. Do you have any wishes?\n",
      "Nick: Hi Marta, yes, I have a few. I'll make some lasagne.\n",
      "Marta: Great!\n",
      "\n",
      "========================== Original Dialogue ================================== \n",
      "Marta: Hi, I'm at the supermarket now to make some shopping for todays dinner.\n",
      "Do you have any wishes?\n",
      "Nick: Hm I don't know.\n",
      "I haven't eat spaghetti in a while Marta: Oh no, I've got spaghetti yesterday by Patric and the day before too.\n",
      "Nick: Okay maybe some fish?\n",
      "Marta: Yeah fish is great, I'll go and search for something Nick: Text me what do you find.\n",
      "Marta: Actually there is one small fish left and I don't think we will be full from it.\n",
      "Nick:\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(56)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_predict     = []\n",
    "val_groundtruth = []\n",
    "for step, batch in enumerate(eval_dataloader):\n",
    "    with torch.no_grad():\n",
    "        generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "            batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        generated_tokens = accelerator.pad_across_processes(\n",
    "            generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = batch[\"labels\"]\n",
    "        if not args.pad_to_max_length:\n",
    "            # If we did not pad to max length, we need to pad the labels too\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "        generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "        labels = accelerator.gather(labels).cpu().numpy()\n",
    "\n",
    "        if args.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        if isinstance(generated_tokens, tuple):\n",
    "            generated_tokens = generated_tokens[0]\n",
    "        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        val_predict.extend(decoded_preds)\n",
    "        val_groundtruth.extend(decoded_labels)\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"Rouge score on val set after epoch {}\".format(epoch+1))\n",
    "eval_results = py_rouge_scores(val_predict, val_groundtruth)\n",
    "val_results.append(val_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a888294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Tom are you busy tomorrow?',\n",
       " 'No Im not, why?',\n",
       " 'Can you go to the animal saving thing?.',\n",
       " 'Why?',\n",
       " 'I wanna get a dog for my son.',\n",
       " 'That will make him very happy.',\n",
       " 'Yeah, we’ve discussed it many times.\\nI think hes ready.',\n",
       " 'That’s good.\\nRaising a dog is tough.\\nLike a baby ;-)',\n",
       " \"I'll get him one of those little dogs.\",\n",
       " \"One that won't grow up too big;-)\",\n",
       " 'And poop all over the place',\n",
       " 'I like to eat chickens everyday?',\n",
       " 'Oh, yes, I took him there last Monday.\\nHe showed me one that he really liked.',\n",
       " 'cuckooo i am a chicken',\n",
       " 'the chicken i want to eat is right at home, away in the fridge',\n",
       " \"I wonder what he'll name it.\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_predict = ['Hi Tom are you busy tomorrow?',\n",
    " 'No Im not, why?',\n",
    " 'Can you go to the animal saving thing?.',\n",
    " 'Why?',\n",
    " 'I wanna get a dog for my son.',\n",
    " 'That will make him very happy.',\n",
    " 'Yeah, we’ve discussed it many times.\\nI think hes ready.',\n",
    " 'That’s good.\\nRaising a dog is tough.\\nLike a baby ;-)',\n",
    " \"I'll get him one of those little dogs.\",\n",
    " \"One that won't grow up too big;-)\",\n",
    " 'And poop all over the place',\n",
    " 'I like to eat chickens everyday?',\n",
    " 'Oh, yes, I took him there last Monday.\\nHe showed me one that he really liked.',\n",
    " 'cuckooo i am a chicken',\n",
    " 'the chicken i want to eat is right at home, away in the fridge',\n",
    " \"I wonder what he'll name it.\"]\n",
    "\n",
    "val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83246771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi Tom, are you busy tomorrow’s afternoon?',\n",
       " 'I’m pretty sure I am.\\nWhat’s up?',\n",
       " 'Can you go with me to the animal shelter?.',\n",
       " 'What do you want to do?',\n",
       " 'I want to get a puppy for my son.',\n",
       " 'That will make him so happy.',\n",
       " 'Yeah, we’ve discussed it many times.\\nI think he’s ready now.',\n",
       " 'That’s good.\\nRaising a dog is a tough issue.\\nLike having a baby ;-)',\n",
       " \"I'll get him one of those little dogs.\",\n",
       " \"One that won't grow up too big;-)\",\n",
       " 'And eat too much;-))',\n",
       " 'Do you know which one he would like?',\n",
       " 'Oh, yes, I took him there last Monday.\\nHe showed me one that he really liked.',\n",
       " 'I bet you had to drag him away.',\n",
       " 'He wanted to take it home right away ;-).',\n",
       " \"I wonder what he'll name it.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_groundtruth = ['Hi Tom, are you busy tomorrow’s afternoon?',\n",
    " 'I’m pretty sure I am.\\nWhat’s up?',\n",
    " 'Can you go with me to the animal shelter?.',\n",
    " 'What do you want to do?',\n",
    " 'I want to get a puppy for my son.',\n",
    " 'That will make him so happy.',\n",
    " 'Yeah, we’ve discussed it many times.\\nI think he’s ready now.',\n",
    " 'That’s good.\\nRaising a dog is a tough issue.\\nLike having a baby ;-)',\n",
    " \"I'll get him one of those little dogs.\",\n",
    " \"One that won't grow up too big;-)\",\n",
    " 'And eat too much;-))',\n",
    " 'Do you know which one he would like?',\n",
    " 'Oh, yes, I took him there last Monday.\\nHe showed me one that he really liked.',\n",
    " 'I bet you had to drag him away.',\n",
    " 'He wanted to take it home right away ;-).',\n",
    " \"I wonder what he'll name it.\"]\n",
    "\n",
    "val_groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "347050b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/17/2022 14:52:48 - INFO - root - \n",
      "06/17/2022 14:52:48 - INFO - root - \tbleu-1: 0.532\tbleu-2: 0.441\tbleu-3: 0.391\tbleu-4: 0.329\n",
      "06/17/2022 14:52:48 - INFO - root - \n"
     ]
    }
   ],
   "source": [
    "bleu = bleu_scores(val_groundtruth, val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20c22da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/17/2022 14:52:59 - INFO - root - \n",
      "06/17/2022 14:52:59 - INFO - root - \tmeteor: 0.540\n",
      "06/17/2022 14:52:59 - INFO - root - \n"
     ]
    }
   ],
   "source": [
    "meteor, scores= meteor_scores(val_groundtruth, val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab97df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5434782608695652,\n",
       " 0.0,\n",
       " 0.66167290886392,\n",
       " 0.0,\n",
       " 0.6320224719101123,\n",
       " 0.8066666666666668,\n",
       " 0.7332282110091743,\n",
       " 0.6843065693430658,\n",
       " 0.9990234375,\n",
       " 0.9985422740524781,\n",
       " 0.11904761904761905,\n",
       " 0.06410256410256411,\n",
       " 0.9998779296875,\n",
       " 0.06493506493506494,\n",
       " 0.33223684210526316,\n",
       " 0.9976851851851852]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c31ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f4c8269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7217391304347825\n"
     ]
    }
   ],
   "source": [
    "reference = list([val_groundtruth[0].split()])\n",
    "# candidate = val_predict[0].split()\n",
    "candidate = list(\"Hi Tom, are you busy tomorrow’s?\".split())\n",
    "score = meteor_score(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d3561f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi', 'Tom,', 'are', 'you', 'busy', 'tomorrow’s', 'afternoon?']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3046a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'Tom,', 'are', 'you', 'busy', \"tomorrow's\", 'afternoon?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53e55a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = \"It is a guide to action which ensures that the military always obeys the commands of the party\"\n",
    "candidate = \"It is a guide to action that ensures that the military will forever heed Party commands\"\n",
    "\n",
    "reference = [reference.split()]\n",
    "candidate = candidate.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cb164c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6320224719101123"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteor_score(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "522f390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bce798fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd360e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0547686614863434e-154\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print(sentence_bleu(reference, candidate, weights=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5322b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = (1./4., 1./4., 1./4., 1./4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c517919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = \"hi\"\n",
    "p = 1\n",
    "r = 2\n",
    "f = 3.333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef9c9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\thi:\tP: 100.00\tR: 200.00\tF1: 333.30\n"
     ]
    }
   ],
   "source": [
    "print('\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:1.2f}'.format(m, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5e09613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.44\n"
     ]
    }
   ],
   "source": [
    "print(\"{:5.2f}\".format(43.444))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910d1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_hlt)",
   "language": "python",
   "name": "conda_hlt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
